{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>第三次作品：學習分類器的評比實驗 </strong>\n",
    "學號：711233112\n",
    "\n",
    "姓名：留筠雅\n",
    "<hr>\n",
    "\n",
    " <font color=skyblue>作品目標</font>：  \n",
    "本次專題計畫旨在比較不同分類器的效果，通過對三組資料進行分類學習和評估。所選用的分類器包括：\n",
    "\n",
    "- 多項式羅吉斯回歸 (Multinomial Logistic Regression)\n",
    "- 支援向量機 (Support Vector Machine)\n",
    "- 神經網絡 (Neural Network)\n",
    "\n",
    "通過這個作品，希望為不同類型的問題提供分類器或是參數設定的選擇建議，並了解各個分類器在不同條件下的運作情況。\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AT & T 的人臉影像資料集\n",
    "人臉影像共 400 張，每張大小 64×64。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "導入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA    \n",
    "from sklearn.svm import SVC,LinearSVC,LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import  GridSearchCV, StratifiedShuffleSplit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "導入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669422</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.491736</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.764463</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.739669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1  0.454545  0.471074  0.512397  0.557851  0.595041  0.640496  0.681818   \n",
       "2  0.318182  0.400826  0.491736  0.528926  0.586777  0.657025  0.681818   \n",
       "3  0.198347  0.194215  0.194215  0.194215  0.190083  0.190083  0.243802   \n",
       "4  0.500000  0.545455  0.582645  0.623967  0.648760  0.690083  0.694215   \n",
       "\n",
       "          7         8         9  ...      4087      4088      4089      4090  \\\n",
       "0  0.677686  0.690083  0.685950  ...  0.669422  0.652893  0.661157  0.475207   \n",
       "1  0.702479  0.710744  0.702479  ...  0.157025  0.136364  0.148760  0.152893   \n",
       "2  0.685950  0.702479  0.698347  ...  0.132231  0.181818  0.136364  0.128099   \n",
       "3  0.404959  0.483471  0.516529  ...  0.636364  0.657025  0.685950  0.727273   \n",
       "4  0.714876  0.723140  0.731405  ...  0.161157  0.177686  0.173554  0.177686   \n",
       "\n",
       "       4091      4092      4093      4094      4095  target  \n",
       "0  0.132231  0.148760  0.152893  0.161157  0.157025       0  \n",
       "1  0.152893  0.152893  0.152893  0.152893  0.152893       0  \n",
       "2  0.148760  0.144628  0.140496  0.148760  0.152893       0  \n",
       "3  0.743802  0.764463  0.752066  0.752066  0.739669       0  \n",
       "4  0.177686  0.177686  0.177686  0.173554  0.173554       0  \n",
       "\n",
       "[5 rows x 4097 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/face_data.csv')\n",
    "n_persons = df['target'].nunique() \n",
    "X = np.array(df.drop('target', axis=1)) # 400 x 4096\n",
    "y = np.array(df['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 切割訓練資料與測試資料\n",
    "2. 進行標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用原始資料訓練多元羅吉斯回歸模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under lbfgs:95.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.75      1.00      0.86         3\n",
      "          38       1.00      0.86      0.92         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.97      0.96       120\n",
      "weighted avg       0.97      0.95      0.95       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under newton-cg:95.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.75      1.00      0.86         3\n",
      "          38       1.00      0.86      0.92         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.97      0.96       120\n",
      "weighted avg       0.97      0.95      0.95       120\n",
      "\n",
      "[LibLinear]accuracy for testing data under liblinear:85.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       0.67      1.00      0.80         2\n",
      "          19       0.33      1.00      0.50         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.25      1.00      0.40         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      0.75      0.86         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      0.75      0.86         4\n",
      "          27       0.75      1.00      0.86         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.33      1.00      0.50         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       0.67      1.00      0.80         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.43      0.60         7\n",
      "          39       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.86       120\n",
      "   macro avg       0.87      0.88      0.85       120\n",
      "weighted avg       0.93      0.86      0.87       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "clf_original_lbfgs= LogisticRegression(solver = 'lbfgs', **opts)# default\n",
    "clf_original_newtoncg= LogisticRegression(solver = 'newton-cg', **opts)\n",
    "clf_original_liblin= LogisticRegression(solver = 'liblinear', **opts)\n",
    "\n",
    "clf_original_lbfgs.fit(X_train_, y_train)\n",
    "y_pred_lbfgs = clf_original_lbfgs.predict(X_test_)\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "print(f'accuracy for testing data under lbfgs:{accuracy_score(y_test, y_pred_lbfgs):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_lbfgs))\n",
    "\n",
    "clf_original_newtoncg.fit(X_train_, y_train)\n",
    "y_pred_new = clf_original_newtoncg.predict(X_test_)\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "print(f'accuracy for testing data under newton-cg:{accuracy_score(y_test, y_pred_new):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_new))\n",
    "\n",
    "clf_original_liblin.fit(X_train_, y_train)\n",
    "y_pred_liblin = clf_original_liblin.predict(X_test_)\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "print(f'accuracy for testing data under liblinear:{accuracy_score(y_test, y_pred_liblin):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_liblin))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- 原始資料訓練出來的模型預測準確率有 95.00%。\n",
    "-  lbfgs, newton-cg, liblinear 這三個演算法裡以lbfgs, newton-cg ( 95.00% ) 預測出來準確率比 liblinear ( 85.83% ) 好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "繪製特徵值由大而小的分佈，如 scree plot 與累積百分比的 pareto plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "pca = PCA(n_components = 20).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "# plot scree plot\n",
    "\n",
    "# Scree plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].plot(np.arange(1, len(pca.explained_variance_ratio_)+1), \n",
    "           pca.explained_variance_ratio_, \n",
    "           marker = 's', \n",
    "           color = \"red\")\n",
    "\n",
    "ax[0].set_xlabel('Principal Components')\n",
    "ax[0].set_ylabel('Eigenvalues')\n",
    "ax[0].set_title('Scree Plot')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Pareto plot\n",
    "x = np.arange(1, 1+len(pca.explained_variance_ratio_))\n",
    "ax[1].bar( x,\n",
    "           pca.explained_variance_ratio_, \n",
    "           color = '#009ad6', \n",
    "           alpha = 0.6, \n",
    "           align = 'center', \n",
    "           label = 'Variance Explained')\n",
    "\n",
    "ax_twinx = ax[1].twinx()\n",
    "ax_twinx.plot(x,\n",
    "              pca.explained_variance_ratio_.cumsum()/pca.explained_variance_ratio_.sum()*100, \n",
    "              marker='s', lw=1, \n",
    "              color=\"red\")\n",
    "\n",
    "ax_twinx.tick_params(axis='y', colors='red')\n",
    "ax_twinx.yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "ax[1].set_xlabel('Principal Component')\n",
    "ax[1].set_ylabel('Variance Explained')\n",
    "ax_twinx.grid()\n",
    "ax[1].set_title('Pareto Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- 觀察 Pareto plot 顯示，前四個主成分已經解釋了約 70 % 的原始變數的變異量。\n",
    "- 接續利用 PCA 後資料訓練的模型，決定取 70% 為成分比例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用主成分分析後的資料訓練多元羅吉斯回歸模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under lbfgs:86.67%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       1.00      0.75      0.86         4\n",
      "          24       0.50      0.67      0.57         3\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       0.80      1.00      0.89         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.75      0.75      0.75         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.50      1.00      0.67         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.60      1.00      0.75         3\n",
      "          38       0.83      0.71      0.77         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.87       120\n",
      "   macro avg       0.87      0.88      0.86       120\n",
      "weighted avg       0.91      0.87      0.87       120\n",
      "\n",
      "accuracy for testing data under newton-cg:86.67%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       1.00      0.75      0.86         4\n",
      "          24       0.50      0.67      0.57         3\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       0.80      1.00      0.89         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.75      0.75      0.75         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.50      1.00      0.67         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.60      1.00      0.75         3\n",
      "          38       0.83      0.71      0.77         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.87       120\n",
      "   macro avg       0.87      0.88      0.86       120\n",
      "weighted avg       0.91      0.87      0.87       120\n",
      "\n",
      "[LibLinear]accuracy for testing data under liblinear:80.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       1.00      0.50      0.67         4\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.25      0.40         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       0.60      1.00      0.75         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          17       0.67      0.67      0.67         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       0.50      1.00      0.67         1\n",
      "          20       0.50      1.00      0.67         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       0.80      1.00      0.89         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.67      1.00      0.80         4\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.25      1.00      0.40         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       0.67      1.00      0.80         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.60      1.00      0.75         3\n",
      "          38       1.00      0.29      0.44         7\n",
      "          39       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.80       120\n",
      "   macro avg       0.84      0.87      0.81       120\n",
      "weighted avg       0.89      0.80      0.79       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.7).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "\n",
    "solver = 'lbfgs' # ’lbfgs’ is the default\n",
    "# solver = ’liblinear’\n",
    "# solver = ’newton−cg’\n",
    "clf_pca_lbfgs= LogisticRegression(solver = 'lbfgs', **opts)# default\n",
    "clf_pca_newtoncg= LogisticRegression(solver = 'newton-cg', **opts)\n",
    "clf_pca_liblin= LogisticRegression(solver = 'liblinear', **opts)\n",
    "\n",
    "clf_pca_lbfgs.fit(Z_train, y_train)\n",
    "y_pred_lbfgs = clf_pca_lbfgs.predict(Z_test)\n",
    "print(f'accuracy for testing data under lbfgs:{clf_pca_lbfgs.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_lbfgs))\n",
    "\n",
    "clf_pca_newtoncg.fit(Z_train, y_train)\n",
    "y_pred_new = clf_pca_newtoncg.predict(Z_test)\n",
    "print(f'accuracy for testing data under newton-cg:{clf_pca_newtoncg.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_new))\n",
    "\n",
    "clf_pca_liblin.fit(Z_train, y_train)\n",
    "y_pred_liblin = clf_pca_liblin.predict(Z_test)\n",
    "print(f'accuracy for testing data under liblinear:{clf_pca_liblin.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred_liblin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- lbfgs, newton-cg, liblinear 這三個演算法裡以 lbfgs, newton-cg ( 86.67\\% ) 預測出來準確率比 liblinear ( 80.00% ) 好。這與利用原始資料訓練的結果一樣，都是 lbfgs, newton-cg 比 liblinear 好。\n",
    "- PCA 後的資料，在這三個模型裡，預測的準確率確實比原始資料的預測準確率來的低，考慮到資料量不大，可以取更高的主成分比例來提高預測準確率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用原始資料訓練 SVM 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=yellow>網格搜索：GridSearchCV</font>**\n",
    "\n",
    "一種調參手段；窮舉搜尋：在所有候選的參數選擇中，透過循環遍歷，嘗試每一種可能性，表現最好的參數就是最終的結果。 其原理就像是在數組裡找到最大值。 這種方法的主要缺點是比較耗時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma': 0.001}\n",
      "0.6928571428571428\n",
      "SVC(C=10, gamma=0.001, max_iter=1000000, tol=1e-06)\n"
     ]
    }
   ],
   "source": [
    "# Get the current date and time\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "\n",
    "now_str = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "results_file = 'C:\\\\Users\\\\f9006\\\\OneDrive\\\\Python_code\\\\ML\\\\data\\\\results_' + now_str + '.csv'\n",
    "\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6)) # parameters for LogisticRegression\n",
    "\n",
    "parameters = {'gamma':[0.001, 0.01, 0.1, 1, 10, 100], 'C':[0.1, 1, 10, 100]} # parameters for GridSearchCV\n",
    "\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, \\\n",
    "\n",
    "                            random_state=0) # 5-fold CV\n",
    "\n",
    "grid = GridSearchCV(estimator=SVC(**opts), \\\n",
    "\n",
    "                param_grid=parameters, cv=cv, \n",
    "\n",
    "                scoring=['accuracy','f1_macro'], refit=\"accuracy\")\n",
    "\n",
    "grid.fit(X_train_, y_train)\n",
    "\n",
    "\n",
    "cv_logistic = pd.DataFrame(data = grid.cv_results_)\n",
    "\n",
    "cv_logistic.to_csv(results_file)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(grid.best_score_)\n",
    "\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "\n",
    "- 根據 param_grid 的值一共會評估 6 x 4 =24 種參數的組合方式，且每一種方式要在訓練集上訓練5次，當訓練結束後可以看到最好的參數組合是 C = 10, gamma = 0.001。\n",
    "- 接下來的 SVM 模型訓練會帶入最佳參數組合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under linear:95.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.86      0.92         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.97      0.95       120\n",
      "weighted avg       0.97      0.95      0.95       120\n",
      "\n",
      "accuracy for testing data under rbf:69.17%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       1.00      0.50      0.67         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.33      0.50         3\n",
      "           7       1.00      0.12      0.22         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      0.67      0.80         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.40      1.00      0.57         2\n",
      "          13       1.00      0.33      0.50         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.33      1.00      0.50         1\n",
      "          22       1.00      0.75      0.86         4\n",
      "          23       1.00      0.75      0.86         4\n",
      "          24       1.00      0.67      0.80         3\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       1.00      0.75      0.86         4\n",
      "          27       1.00      0.33      0.50         3\n",
      "          28       1.00      0.75      0.86         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.14      0.25         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.69       120\n",
      "   macro avg       0.93      0.75      0.78       120\n",
      "weighted avg       0.97      0.69      0.75       120\n",
      "\n",
      "accuracy for testing data under poly:60.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29         5\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       0.33      0.25      0.29         4\n",
      "           4       0.50      0.67      0.57         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.33      0.50         3\n",
      "           7       1.00      0.25      0.40         8\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       0.67      0.67      0.67         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.40      0.57         5\n",
      "          12       0.08      1.00      0.15         2\n",
      "          13       1.00      0.33      0.50         3\n",
      "          14       0.60      1.00      0.75         3\n",
      "          15       0.67      0.67      0.67         3\n",
      "          17       1.00      0.33      0.50         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.17      1.00      0.29         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      0.50      0.67         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.50      0.67      0.57         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      0.25      0.40         4\n",
      "          27       1.00      0.33      0.50         3\n",
      "          28       1.00      0.75      0.86         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.00      0.00      0.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.00      0.00      0.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      0.67      0.80         3\n",
      "          38       1.00      0.43      0.60         7\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.61       120\n",
      "   macro avg       0.77      0.65      0.65       120\n",
      "weighted avg       0.83      0.61      0.64       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under LinearSVC:85.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       0.67      0.67      0.67         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       0.50      1.00      0.67         2\n",
      "          19       0.33      1.00      0.50         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.25      1.00      0.40         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      0.75      0.86         4\n",
      "          27       0.75      1.00      0.86         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.33      1.00      0.50         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       0.67      1.00      0.80         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       0.67      1.00      0.80         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.29      0.44         7\n",
      "          39       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.85       120\n",
      "   macro avg       0.85      0.88      0.83       120\n",
      "weighted avg       0.92      0.85      0.85       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "C = 10 # SVM regularization parameter\n",
    "opts_rest = dict(C = C, tol = 1e-6, max_iter = int(1e6))\n",
    "opts = dict(C = C, decision_function_shape = 'ovo', \\\n",
    "tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm_linear = SVC(kernel=\"linear\", **opts)\n",
    "clf_svm_rbf = SVC(kernel=\"rbf\", gamma=0.001, **opts)\n",
    "clf_svm_poly = SVC(kernel=\"poly\", degree=3, gamma=\"auto\", **opts)\n",
    "clf_svm_rest = LinearSVC(**opts_rest) # one vs the rest\n",
    "#para = {\"kernal\":[\"linear\", \"rbf\", \"poly\"], \"C\":[0.1, 1, 10], \"gamma\":[0.1, 1, 10]}\n",
    "clf_svm_linear.fit(X_train_,y_train)\n",
    "predictions=clf_svm_linear.predict(X_test_)\n",
    "print(f'accuracy for testing data under linear:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_rbf.fit(X_train_,y_train)\n",
    "predictions=clf_svm_rbf.predict(X_test_)\n",
    "print(f'accuracy for testing data under rbf:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_poly.fit(X_train_,y_train)\n",
    "predictions=clf_svm_poly.predict(X_test_)\n",
    "print(f'accuracy for testing data under poly:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_rest.fit(X_train_,y_train)\n",
    "predictions=clf_svm_rest.predict(X_test_)\n",
    "print(f'accuracy for testing data under LinearSVC:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- linear 是這四個 SVM 模型裡預測準確率最高的( 95.00% )，LinearSVC 也不錯有 85% 的準確率，其他的核函數預測效果皆很差，推測是資料類型不適合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用主成分分析後資料訓練 SVM 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under linear:89.17%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       0.50      1.00      0.67         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.75      1.00      0.86         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.75      0.75      0.75         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.75      1.00      0.86         3\n",
      "          38       0.83      0.71      0.77         7\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.89       120\n",
      "   macro avg       0.91      0.93      0.90       120\n",
      "weighted avg       0.92      0.89      0.89       120\n",
      "\n",
      "accuracy for testing data under rbf:85.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.50      0.67         8\n",
      "           8       0.50      1.00      0.67         2\n",
      "           9       0.50      1.00      0.67         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.50      1.00      0.67         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      0.75      0.86         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      0.67      0.80         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.75      0.75      0.75         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.83      0.71      0.77         7\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.85       120\n",
      "   macro avg       0.87      0.87      0.85       120\n",
      "weighted avg       0.90      0.85      0.85       120\n",
      "\n",
      "accuracy for testing data under poly:72.50%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29         5\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.67      0.67      0.67         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.33      0.50         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      0.67      0.80         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.40      1.00      0.57         2\n",
      "          13       1.00      0.33      0.50         3\n",
      "          14       0.17      1.00      0.29         3\n",
      "          15       0.50      1.00      0.67         3\n",
      "          17       0.67      0.67      0.67         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.50      1.00      0.67         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      0.50      0.67         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      0.75      0.86         4\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       0.67      1.00      0.80         4\n",
      "          29       1.00      0.67      0.80         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.00      0.00      0.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.50      0.33      0.40         3\n",
      "          38       1.00      0.43      0.60         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.81      0.76      0.75       120\n",
      "weighted avg       0.83      0.72      0.74       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under LinearSVC:75.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      1.00      0.67         2\n",
      "           3       1.00      0.50      0.67         4\n",
      "           4       0.67      0.67      0.67         3\n",
      "           5       0.60      1.00      0.75         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       0.67      0.67      0.67         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       0.43      1.00      0.60         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.67      0.67      0.67         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.50      1.00      0.67         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       0.75      0.75      0.75         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       0.50      0.50      0.50         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      0.33      0.50         3\n",
      "          28       1.00      0.50      0.67         4\n",
      "          29       0.67      0.67      0.67         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.33      1.00      0.50         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       0.67      1.00      0.80         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       0.67      1.00      0.80         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.75      0.43      0.55         7\n",
      "          39       0.50      0.25      0.33         4\n",
      "\n",
      "    accuracy                           0.75       120\n",
      "   macro avg       0.78      0.79      0.75       120\n",
      "weighted avg       0.83      0.75      0.75       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components = 0.7).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "\n",
    "clf_svm_linear.fit(Z_train,y_train)\n",
    "predictions=clf_svm_linear.predict(Z_test)\n",
    "print(f'accuracy for testing data under linear:{clf_svm_linear.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_rbf.fit(Z_train,y_train)\n",
    "predictions=clf_svm_rbf.predict(Z_test)\n",
    "print(f'accuracy for testing data under rbf:{clf_svm_rbf.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_poly.fit(Z_train,y_train)\n",
    "predictions=clf_svm_poly.predict(Z_test)\n",
    "print(f'accuracy for testing data under poly:{clf_svm_poly.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_svm_rest.fit(Z_train,y_train)\n",
    "predictions=clf_svm_rest.predict(Z_test)\n",
    "print(f'accuracy for testing data under LinearSVC:{clf_svm_rest.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- linear 是這四個 SVM 模型裡預測準確率最高的( 89.17% )，這四個模型準確率都有 72% 以上。\n",
    "- 有兩個模型的預測準確率比原始資料訓練的模型高，推測是有做 PCA 的資料對於未知的資料可能預測能力會更好一些，畢竟不會受其他不重要的特徵影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用原始資料訓練 ANN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under sgd:94.17%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.60      1.00      0.75         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.80      1.00      0.89         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.86      0.92         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.94       120\n",
      "   macro avg       0.95      0.96      0.95       120\n",
      "weighted avg       0.96      0.94      0.94       120\n",
      "\n",
      "accuracy for testing data under lbfgs:90.83%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       0.67      0.50      0.57         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.50      1.00      0.67         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       0.75      1.00      0.86         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       0.50      1.00      0.67         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      0.86      0.92         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.91       120\n",
      "   macro avg       0.93      0.94      0.92       120\n",
      "weighted avg       0.93      0.91      0.91       120\n",
      "\n",
      "accuracy for testing data under adam:90.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       0.80      1.00      0.89         4\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.50      0.67         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.60      1.00      0.75         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.33      1.00      0.50         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.86      0.86      0.86         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.90       120\n",
      "   macro avg       0.89      0.91      0.88       120\n",
      "weighted avg       0.94      0.90      0.90       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# hidden_layers = (512,) # one hidden layer\n",
    "# activation = 'relu' # the default\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers , verbose = False, \\\n",
    "activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "\n",
    "clf_MLP_sgd = MLPClassifier(solver = 'sgd', **opts) \n",
    "clf_MLP_lbfgs = MLPClassifier(solver = 'lbfgs', **opts) \n",
    "clf_MLP_adam = MLPClassifier(solver = 'adam', **opts) \n",
    "\n",
    "clf_MLP_sgd.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_sgd.predict(X_test_)\n",
    "print(f'accuracy for testing data under sgd:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_MLP_lbfgs.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_lbfgs.predict(X_test_)\n",
    "print(f'accuracy for testing data under lbfgs:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_MLP_adam.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_adam.predict(X_test_)\n",
    "print(f'accuracy for testing data under adam:{accuracy_score(y_test, predictions):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- 運算時間比多元羅吉斯回歸和 SVM 學習器來得久，約花4分鐘。\n",
    "- 這三個 ANN 模型裡以 sgd 演算法預測出來的準確率最高( 94.17% )，所有模型都有 90% 以上的準確率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用主成分分析後的資料訓練 ANN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under sgd:83.33%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33         5\n",
      "           1       0.80      1.00      0.89         4\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       0.67      0.50      0.57         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.62      0.77         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       0.60      1.00      0.75         3\n",
      "          10       1.00      0.67      0.80         3\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       0.75      1.00      0.86         3\n",
      "          14       0.75      1.00      0.86         3\n",
      "          15       0.75      1.00      0.86         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      0.67      0.80         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.75      0.75      0.75         4\n",
      "          23       1.00      0.75      0.86         4\n",
      "          24       0.60      1.00      0.75         3\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.60      0.75      0.67         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       0.50      1.00      0.67         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.60      1.00      0.75         3\n",
      "          38       0.83      0.71      0.77         7\n",
      "          39       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.83       120\n",
      "   macro avg       0.86      0.86      0.84       120\n",
      "weighted avg       0.89      0.83      0.83       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under lbfgs:82.50%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      0.50      0.50         2\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.67      0.67      0.67         3\n",
      "           7       1.00      0.75      0.86         8\n",
      "           8       0.67      1.00      0.80         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.40      1.00      0.57         2\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       0.67      1.00      0.80         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       0.50      1.00      0.67         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       0.80      1.00      0.89         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       0.43      0.75      0.55         4\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.50      0.14      0.22         7\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.82       120\n",
      "   macro avg       0.84      0.84      0.82       120\n",
      "weighted avg       0.87      0.82      0.82       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for testing data under adam:84.17%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      0.50      0.67         4\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       1.00      0.62      0.77         8\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       0.75      1.00      0.86         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.75      1.00      0.86         3\n",
      "          18       0.67      1.00      0.80         2\n",
      "          19       0.50      1.00      0.67         1\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.50      0.67      0.57         3\n",
      "          25       0.67      1.00      0.80         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       0.50      0.75      0.60         4\n",
      "          29       0.75      1.00      0.86         3\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.75      0.43      0.55         7\n",
      "          39       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.84       120\n",
      "   macro avg       0.85      0.88      0.84       120\n",
      "weighted avg       0.88      0.84      0.84       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\f9006\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "clf_MLP_sgd.fit(Z_train, y_train)  \n",
    "predictions = clf_MLP_sgd.predict(Z_test)\n",
    "print(f'accuracy for testing data under sgd:{clf_MLP_sgd.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_MLP_lbfgs.fit(Z_train, y_train)\n",
    "predictions = clf_MLP_lbfgs.predict(Z_test)\n",
    "print(f'accuracy for testing data under lbfgs:{clf_MLP_lbfgs.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "clf_MLP_adam.fit(Z_train, y_train)\n",
    "predictions = clf_MLP_adam.predict(Z_test)\n",
    "print(f'accuracy for testing data under adam:{clf_MLP_adam.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=skyblue>對執行結果的觀察紀錄：</font>**\n",
    "- 有經過 PCA 的資料訓練出來的模型運算時間比原始資料來得快速，約花 30 秒。\n",
    "- 這三個 ANN 模型裡以 adam 演算法預測出來的準確率最高( 84.17% )，但不同演算法預測出的模型準確率，差別不大。\n",
    "-  整體來說，有 PCA 過後的資料有降低些許預測準確率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### **<font color=\t#2E8B57 >原始資料的模型訓練觀察結果</font>**\n",
    "比較 3 組學習器的模型預測準確率，從每個學習器裡取最高測準確率，在原始資料中以多項式羅吉斯回歸和支援向量機學習器預測準確率最高。\n",
    " - 多項式羅吉斯回歸 95.00%\n",
    " - 支援向量機 95.00%\n",
    " - 神經網路 94.17%\n",
    "  \n",
    "#### **<font color=\t#2E8B57 >主成分分析後的資料的模型訓練觀察結果</font>**\n",
    "在 PCA 資料中以支援向量機學習器預測準確率最高。\n",
    " - 多項式羅吉斯回歸 86.67%\n",
    " - 支援向量機 89.17%\n",
    " - 神經網路 84.17%\n",
    "  \n",
    "#### **<font color=#FA8072> 整體觀察結果：</font>**\n",
    "1. 以 AT&T 資料集來說，這三種學習分類器使用原始資料來預測的效果皆不錯，但要注意在相同預測正確率之下， ANN 模型須花較多時間訓練。\n",
    "2. 有主成分分析的資料整體而言會降低一些模型預測準確率。\n",
    "3. 學習器內不同參數的調整對模型訓練的效果影響巨大。\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
